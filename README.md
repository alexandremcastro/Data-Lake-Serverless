## Sum√°rio
+ [Introdu√ß√£o](#Introducao)
    + [Links](#Links)
        + [Link para o projeto no Notion](#Notion)
        + [Link para o projeto no Site](#Site)
    + [Arquitetura do projeto](#Arquitetura-do-projeto)
    + [Criando o servi√ßo Cloud9](#Cloud9)
    + [Rodando o Producer e Consumer](#ProducerConsumer)
+ [Pipeline com dados n√£o estruturados](#PipelineNaoEstruturado)
    + [Criando o servi√ßo Kinesis Data Streams](#CriacaoDataStreams)
    + [Criando uma tabela no DynamoDB](#CriacaoDynamoDB)
    + [Administrando pol√≠ticas de seguran√ßa com o IAM](#IAM)
        + [Cria√ß√£o da pol√≠tica](#Politica)
        + [Cria√ß√£o da fun√ß√£o](#Funcao)
    + [Criando uma fun√ß√£o com o servi√ßo Lambda](#FuncaoLambda)
    + [Processando dados n√£o estruturados (Data Streams, DynamoDB e Lambda)](#ProcessandoNaoEstruturado)
+ [Pipeline com dados estruturados](#PipelineEstruturados)
    + [Criando um bucket S3](#S3)
    + [Criando uma tabela no Glue](#Glue)
    + [Criando o servi√ßo Kinesis Data Firehose](#Firehose)
    + [Processando dados estruturados (S3, Glue e Data Firehose)](#ProcessandoEstruturados)
    + [Criando e fazendo consultas com o Athena](#Athena)

# Introdu√ß√£o <a name = "Introducao"></a>

## Links <a name = "Links"></a>
Link para o projeto no 
[Notion](https://alexandremcastro.notion.site/02-2023-AWS-Data-Lake-Serverless-c8f8198221134364991976c26ee1a985)  <a name = "Notion"></a>

Link para o projeto no
[Site](https://alexandre-castro.vercel.app/blog/datalake-serverless) <a name = "Site"></a>

## Arquitetura do projeto <a name = "Arquitetura-do-projeto"></a>

A proposta desse projeto √© fornecer um Data Lake em nuvem, Serverless, utilizando os servi√ßos da Amazon Web Services.

![Diagrama sem nome.jpg](Imagens/Diagrama_sem_nome.jpg)

<br>

## Criando o servi√ßo Cloud9 <a name = "Cloud9"></a>

Para que √© usado o servi√ßo Cloud9? Ele fornece um terminal de comando para uma inst√¢ncia EC2. Isso √© essencial neste projeto, pois ser√° nele que ser√£o gerados os dados para consumo

V√° no painel e na pesquisa procure e clique no servi√ßo Cloud9

![Untitled](Imagens/Untitled.png)

Voc√™ ser√° redirecionado a essa tela

Clique em: `Criar ambiente`

![Untitled](Imagens/Untitled%201.png)

Coloque um nome para identifica√ß√£o, a descri√ß√£o √© opcional

Mantenha as outras op√ß√µes em padr√£o

![Untitled](Imagens/Untitled%202.png)

<aside>
üí° Verifique se a inst√¢ncia que est√° sendo criada √© coberta pelo n√≠vel gratuito.
</aside>

<br>

![Untitled](Imagens/Untitled%203.png)

Para criar a o Cloud9, clique em `Criar`

![Untitled](Imagens/Untitled%204.png)

Para acessar o terminal criado pelo Cloud9, clique em: `Em aberto`

![Untitled](Imagens/Untitled%205.png)

A cria√ß√£o do servi√ßo demora alguns minutos, pois ainda ser√° criado o servi√ßo EC2 simultaneamente. Ao criar voc√™ ser√° redirecionado para essa tela:

![Untitled](Imagens/Untitled%206.png)

√â importante manter essa tela aberta at√© o fim do projeto, nela que ser√° rodado o script respons√°vel por gerar os dados para o Data Lake

<br>

## Rodando o Producer e Consumer <a name = "ProducerConsumer"></a> 
### Baixando o Producer e Consumer 

Para baixar o Producer/Consumer, abra a janela com o terminal do Cloud9 e rode o comando:

```bash
curl -s [https://data-processing.serverlessworkshops.io/client/client.tar](https://data-processing.serverlessworkshops.io/client/client.tar) | tar -xv
```

![Untitled](Imagens/Untitled%207.png)

Essa etapa ser√° repetida algumas vezes, ent√£o √© de extrema import√¢ncia saber como realizar ela at√© o fim do projeto

<br>

### Producer

O Producer √© respons√°vel por simular os dados dos sensores nos cavalos, √© um script fornecido pela AWS feito em Go, j√° compilado.

Para come√ßar gerar os dados, abra o terminal do Cloud9 e rode o comando:

```bash
./producer
```

![Untitled](Imagens/Untitled%208.png)

<br>

### Consumer

O Consumer √© respons√°vel por consumir os dados dos sensores nos cavalos, neste caso, o Consumer est√° lendo os dados gerados pelo Producer, o Consumer tamb√©m √© um script fornecido pela AWS feito em Go, j√° compilado.

Para verificar se est√° tudo funcionando corretamente, abra uma nova janela do terminal e rode o comando:

```bash
./consumer
```

![Untitled](Imagens/Untitled%209.png)

Para parar de rodar ambos, fa√ßa o comando: `CTRL + C`

<br>

# Pipeline com dados n√£o estruturados <a name = "PipelineNaoEstruturado"></a>

## Criando o servi√ßo Kinesis Data Streams <a name = "CriacaoDataStreams"></a>

Para que √© usado o servi√ßo Kinesis? O AWS Kinesis √© um servi√ßo de streaming de dados, usado para coletar, processar e analisar grandes volumes de dados em tempo real.

O Kinesis Data Streams ser√° necess√°rio no projeto para capturar os dados gerados pelo Producer.

V√° no painel e na barra pesquisa procure por `Kinesis`

Clique em: `Kinesis`

![Untitled](Imagens/Untitled%2010.png)

Nesta tela, selecione: `Kinesis Data Streams` 

Em seguida, selecione: `Criar fluxo de dados`

![Untitled](Imagens/Untitled%2011.png)

Insira o nome do fluxo de dados como: `wildrydes`

Selecione a capacidade do fluxo de dados para: `Provisionado`

Mantenha as outras op√ß√µes em padr√£o

Em seguida clique em: `Criar fluxo de dados`

![Untitled](Imagens/Untitled%2012.png)

<br>

## Criando uma tabela no DynamoDB <a name = "CriacaoDynamoDB"></a>

Para que √© usado o servi√ßo DynamoDB? O AWS DynamoDB √© um servi√ßo de banco de dados NoSQL, que permite armazenar e recuperar grandes volumes de dados de forma r√°pida e escal√°vel, sem a necessidade de provisionar ou gerenciar servidores. Ele ser√° uma das fontes de armazenamentos de dados do Data Lake.

V√° no painel e na barra pesquisa procure por `DynamoDB`

Clique em: `DynamoDB`

![Untitled](Imagens/Untitled%2013.png)

Para criar a tabela que receberemos os dados, clique em: `Criar tabela`

![Untitled](Imagens/Untitled%2014.png)

Como nome da tabela utilize o nome: `UnicornSensorData`

Como chave de parti√ß√£o utilize: `Name`

Como chave de classifica√ß√£o utilize: `StatusTime`

![Untitled](Imagens/Untitled%2015.png)

As pr√≥ximas op√ß√µes s√£o padr√£o

Ao fim da p√°gina, clique em: `Criar tabela`

![Untitled](Imagens/Untitled%2016.png)

<br>

## Administrando pol√≠ticas de seguran√ßa com o IAM <a name = "IAM"></a>

Para que √© usado o servi√ßo IAM? O AWS IAM (Identity and Access Management) √© um servi√ßo de gerenciamento de identidade e acesso, usado para gerenciar o acesso a recursos da AWS de forma segura, permitindo gerenciar usu√°rios, grupos e permiss√µes de acesso aos servi√ßos.

V√° no painel e na barra pesquisa procure por `IAM`

Clique em: `IAM`

![Untitled](Imagens/Untitled%2017.png)

<br>

### Cria√ß√£o da pol√≠tica <a name = "Politica"></a>

Criarei uma politica para o DynamoDB permitir a grava√ß√£o de dados na tabela criada.

Na p√°gina redirecionada, clique em: `Politicas`

![Untitled](Imagens/Untitled%2018.png)

Em seguida clique em: `Criar politica`

![Untitled](Imagens/Untitled%2019.png)

No servi√ßo, selecione: `DynamoDB`

Nas a√ß√µes, selecione: `BatchWriteItem`

![Untitled](Imagens/Untitled%2020.png)

Em recursos, selecione: `Adicionar ARN`

![Untitled](Imagens/Untitled%2021.png)

Para preencher essa tabela, √© necess√°rio do c√≥digo ARN da tabela criada no DynamoDB

![Untitled](Imagens/Untitled%2022.png)

Para ter acesso ao ARN da tabela no DynamoDB, v√° no servi√ßo e clique na tabela que foi criada

![Untitled](Imagens/Untitled%2023.png)

O ARN fica localizado nas informa√ß√µes gerais da tabela

Copie o c√≥digo ARN

![Untitled](Imagens/Untitled%2024.png)

Cole o c√≥digo ARN e todas as informa√ß√µes ser√£o preenchidas automaticamente

Clique em: `Adicionar`

![Untitled](Imagens/Untitled%2025.png)

Prosseguindo a cria√ß√£o da pol√≠tica

Clique em: `Pr√≥ximo: Tags`

![Untitled](Imagens/Untitled%2026.png)

Clique em: `Pr√≥ximo: Revisar`

![Untitled](Imagens/Untitled%2027.png)

Em nome, insira: `WildRydesDynamoDBWritePolicy`

Ap√≥s, clique em: `Criar pol√≠tica`

![Untitled](Imagens/Untitled%2028.png)

<br>

### Cria√ß√£o da fun√ß√£o <a name = "Funcao"></a>

Agora irei criar uma fun√ß√£o para o Lambda se comunicar com o Kinesis e DynamoDB utilizando as politicas:`WildRydesDynamoDBWritePolicy` e `AWSLambdaKinesisExecutionRole`

Clique em: `Fun√ß√µes`

Depois clique em: `Criar fun√ß√£o`

![Untitled](Imagens/Untitled%2029.png)

Em caso de uso, selecione `Lambda`

Depois clique em: `Pr√≥ximo`

![Untitled](Imagens/Untitled%2030.png)

Pesquise pela pol√≠tica `AWSLambdaKinesisExecutionRole`

Selecione a pol√≠tica: `AWSLambdaKinesisExecutionRole`

![Untitled](Imagens/Untitled%2031.png)

Pesquise pela pol√≠tica `WildRydesDynamoDBWritePolicy`

Selecione a pol√≠tica: `WildRydesDynamoDBWritePolicy`

![Untitled](Imagens/Untitled%2032.png)

Confirme se as duas pol√≠ticas est√£o selecionadas

![Untitled](Imagens/Untitled%2033.png)

Prossiga a cria√ß√£o da fun√ß√£o

Selecione: `Pr√≥ximo`

![Untitled](Imagens/Untitled%2034.png)

Em nome da fun√ß√£o, insira: `WildRydesStreamProcessorRole`

![Untitled](Imagens/Untitled%2035.png)

Verifique as pol√≠ticas selecionadas

Selecione: `Criar fun√ß√£o`

![Untitled](Imagens/Untitled%2036.png)

Verifique se a fun√ß√£o foi criada

![Untitled](Imagens/Untitled%2037.png)

<br>

## Criando uma fun√ß√£o com o servi√ßo Lambda <a name = "FuncaoLambda"></a>

Para que √© usado o servi√ßo Lambda? √â um servi√ßo para rodar c√≥digos de diversas linguagens diferentes, nele rodar√° o c√≥digo que ser√° respons√°vel por trazer os dados do Kinesis e levar para o DynamoDB.

V√° no painel e na barra pesquisa procure por `Lambda`

Clique em: `Lambda`

![Untitled](Imagens/Untitled%2038.png)

Clique em: `Criar uma fun√ß√£o`

![Untitled](Imagens/Untitled%2039.png)

Em nome da fun√ß√£o, insira o nome: `WildRydesStreamProcessor`

Selecione abaixo a op√ß√£o de execu√ß√£o: `Node.js 14.x`

![Untitled](Imagens/Untitled%2040.png)

No papel de execu√ß√£o, selecione a op√ß√£o: `Usar uma fun√ß√£o existente`

Na fun√ß√£o, selecione a `WildRydesStreamProcessorRole`, criada anteriormente na se√ß√£o do IAM

Em seguida, selecione: `Criar fun√ß√£o`

![Untitled](Imagens/Untitled%2041.png)

Na parte de c√≥digo, insira o c√≥digo a seguir:

```jsx
"use strict";

const AWS = require("aws-sdk");
const dynamoDB = new AWS.DynamoDB.DocumentClient();
const tableName = process.env.TABLE_NAME;

exports.handler = function (event, context, callback) {
    const requestItems = buildRequestItems(event.Records);
    const requests = buildRequests(requestItems);

    Promise.all(requests)
        .then(() =>
            callback(null, `Delivered ${event.Records.length} records`)
        )
        .catch(callback);
};

function buildRequestItems(records) {
    return records.map((record) => {
        const json = Buffer.from(record.kinesis.data, "base64").toString(
            "ascii"
        );
        const item = JSON.parse(json);

        return {
            PutRequest: {
                Item: item,
            },
        };
    });
}

function buildRequests(requestItems) {
    const requests = [];
    // Batch Write 25 request items from the beginning of the list at a time
    while (requestItems.length > 0) {
        const request = batchWrite(requestItems.splice(0, 25));

        requests.push(request);
    }

    return requests;
}

function batchWrite(requestItems, attempt = 0) {
    const params = {
        RequestItems: {
            [tableName]: requestItems,
        },
    };

    let delay = 0;

    if (attempt > 0) {
        delay = 50 * Math.pow(2, attempt);
    }

    return new Promise(function (resolve, reject) {
        setTimeout(function () {
            dynamoDB
                .batchWrite(params)
                .promise()
                .then(function (data) {
                    if (data.UnprocessedItems.hasOwnProperty(tableName)) {
                        return batchWrite(
                            data.UnprocessedItems[tableName],
                            attempt + 1
                        );
                    }
                })
                .then(resolve)
                .catch(reject);
        }, delay);
    });
}
```

![Untitled](Imagens/Untitled%2042.png)

Antes do Deploy, v√° nas configura√ß√µes de vari√°veis de ambiente

Clique em: `Editar`

![Untitled](Imagens/Untitled%2043.png)

**Primeira vari√°vel de ambiente**

Clique em: `Adicionar vari√°veis de ambiente`

Na primeira vari√°vel de ambiente, insira:

Chave: `TABLE_NAME`

Valor: `UnicornSensorData`

<br>

**Segunda vari√°vel de ambiente**

Clique em: `Adicionar vari√°veis de ambiente`

Na segunda vari√°vel de ambiente, insira:

Chave: `AWS_NODEJS_CONNECTION_REUSE_ENABLED`

Valor: `1`

Clique em: `Salvar`

![Untitled](Imagens/Untitled%2044.png)

Adicionando o Kinesis na fun√ß√£o

Clique em: `Adicionar gatilho`

![Untitled](Imagens/Untitled%2045.png)

Selecione o servi√ßo: `Kinesis` 

Em Stream do Kinesis, insira o stream criado anteriormente no Kinesis Data Streams

![Untitled](Imagens/Untitled%2046.png)

Mantenha o tamanho do lote em 100

Clique em: `Adicionar`

![Untitled](Imagens/Untitled%2047.png)

Com tudo configurado, farei o Deploy da fun√ß√£o

V√° na aba C√≥digo e clique em: `Deploy`

![Untitled](Imagens/Untitled%2048.png)

<br>

## Processando dados n√£o estruturados **(Data Streams, DynamoDB e Lambda)** <a name = "ProcessandoNaoEstruturado"></a>

Com o Kinesis, DynamoDB, IAM e Lambda criados e configurados, agora √© poss√≠vel fazer o processamento dos dados gerados pelo script Producer.

Com o Cloud9 aberto, rode o Producer, mencionando o stream criado no Kinesis

```bash
./producer -stream wildrydes
```

Rode o Consumer e verifique a produ√ß√£o dos dados

![Untitled](Imagens/Untitled%2049.png)

Na aba `Monitor`, verifique se os dados est√£o sendo capturados

![Untitled](Imagens/Untitled%2050.png)

Para verificar se os dados est√£o sendo entregues, abra o servi√ßo `DynamoDB`

V√° na aba tabelas e selecione a tabela `UnicornSensorData`

![Untitled](Imagens/Untitled%2051.png)

Clique em: `Explorar itens da tabela`

![Untitled](Imagens/Untitled%2052.png)

Verifique se os dados foram preenchidos

![Untitled](Imagens/Untitled%2053.png)

<br>

# Pipeline com dados estruturados <a name = "PipelineEstruturados"></a>

## Criando um bucket S3 <a name = "S3"></a>

Para que √© usado o servi√ßo S3? O S3 √© um sistema de armazenamento, nele pode ser armazenado qualquer tipo de arquivo

Depois de ter enviado os dados para o DynamoDB, temos a op√ß√£o de enviar eles para o S3

V√° no painel e na barra pesquisa procure por `S3`

Clique em: `S3`

![Untitled](Imagens/Untitled%2054.png)

Clique em: `Criar bucket`

![Untitled](Imagens/Untitled%2055.png)

<aside>
üí° Ao nomear o bucket, de um nome inexistente at√© o momento, o nome do bucket deve ser globalmente exclusivo.

</aside>

Insira o nome do bucket

Em Regi√£o da AWS, selecione: `Leste dos EUA¬†(Norte da Virg√≠nia) us-east-1`

![Untitled](Imagens/Untitled%2056.png)

Mantenha as outras configura√ß√µes padr√µes

Clique em: `Criar bucket`

![Untitled](Imagens/Untitled%2057.png)

<br>

## Criando uma tabela no Glue <a name = "Glue"></a>

Para que √© usado o servi√ßo Glue? O Glue √© um servi√ßo de integra√ß√£o de dados, nele ser√° poss√≠vel capturar os dados vindo do Kinesis e levando ele ao bucket S3.

V√° no painel e na barra pesquisa procure por `Glue`

Clique em: `AWS Glue`

![Untitled](Imagens/Untitled%2058.png)

V√° na aba `Tables`, em Data Catalog

Clique em: `Add table`

![Untitled](Imagens/Untitled%2059.png)

Em Table details, insira um nome para a tabela

Em Database, selecione: `default`

<aside>
üí° Caso n√£o apare√ßa nenhum banco de dados, crie um no bot√£o: `Create database`

</aside>

![Untitled](Imagens/Untitled%2060.png)

Em Data Store, selecione a op√ß√£o: `S3`

Depois selecione onde est√° localizado o bucket, clique em: `Browse S3`

![Untitled](Imagens/Untitled%2061.png)

Selecione o bucket criado anteriormente

Clique em: `Choose`

![Untitled](Imagens/Untitled%2062.png)

Em Data format, selecione o formato: `Parquet`

Clique em: `Next`

![Untitled](Imagens/Untitled%2063.png)

Para definir o schema, clique em: `Add`

![Untitled](Imagens/Untitled%2064.png)

O schema deve ser baseado nas informa√ß√µes geradas pelo Producer

![Untitled](Imagens/Untitled%2065.png)

Insira cada coluna do schema, seguindo a ordem de sa√≠da gerada pelo Producer e seguindo os tipos de dados.

![Untitled](Imagens/Untitled%2066.png)

Repita esse processo para todas as colunas

| Coluna | Tipo |
| --- | --- |
| Distance | float |
| HealthPoints | int |
| Latitude | float |
| Longitude | float |
| MagicPoints | int |
| Name | string |
| InputData | string |
| StatusTime | timestamp |

![Untitled](Imagens/Untitled%2067.png)

Mantenha as outras configura√ß√µes em padr√£o

Clique em: `Next`

![Untitled](Imagens/Untitled%2068.png)

Verifique se tudo est√° corretamente configurado

Clique em: `Create`

![Untitled](Imagens/Untitled%2069.png)

<br>

## Criando o servi√ßo Kinesis Data Firehose <a name = "Firehose"></a>

V√° no painel e na barra pesquisa procure por `Kinesis`

Clique em: `Kinesis`

![Untitled](Imagens/Untitled%2010.png)

Clique em: `Criar stream de entrega`

![Untitled](Imagens/Untitled%2070.png)

Na origem, selecione: `Amazon Kinesis Data Streams`

No destino, selecione: `Amazon S3`

![Untitled](Imagens/Untitled%2071.png)

Na configura√ß√£o da origem, clique em: `Browse`

![Untitled](Imagens/Untitled%2072.png)

Selecione o Data stream criado anteriormente

Clique em: `Choose`

![Untitled](Imagens/Untitled%2073.png)

Insira um nome para o fluxo de entrega

![Untitled](Imagens/Untitled%2074.png)

Em transformar e converter registros, selecione: `Ativar convers√£o de formato de registro`

Em formato de sa√≠da, selecione: `Apache Parquet`

Em regi√£o, selecione `Leste dos EUA (Norte da Virg√≠nia)`

Em banco de dados do AWS Glue, selecione o banco de dados criado no Glue

Em tabela do AWS Glue, selecione a tabela criada no Glue

![Captura da Web_14-3-2023_185721_us-east-1.console.aws.amazon.com.jpeg](Imagens/Captura_da_Web_14-3-2023_185721_us-east-1.console.aws.amazon.com.jpeg)

Em configura√ß√µes do destino, clique em: `Browse`

![Untitled](Imagens/Untitled%2075.png)

Selecione o bucket criado anteriormente

Clique em: `Choose`

![Untitled](Imagens/Untitled%2076.png)

Em intervalo do buffer, coloque `60`

![Untitled](Imagens/Untitled%2077.png)

Clique em: `Criar fluxo de entrega`

![Untitled](Imagens/Untitled%2078.png)

<br>

## Processando dados estruturados **(S3, Glue e Data Firehose)** <a name = "ProcessandoEstruturados"></a>

Com tudo configurado, agora √© os dados gerados pelo Producer, ser√£o enviados diretamente para o S3.

Para isso, rode o Producer indicando o nome do Stream

```bash
./producer -stream wildrydes
```

Deixe alguns minutos rodando

Acesse o bucket criado e cheque se foi criado um diret√≥rio

![Untitled](Imagens/Untitled%2079.png)

Siga at√© o √∫ltimo destino e verifique se l√° possuem os objetos `.parquet`

![Untitled](Imagens/Untitled%2080.png)

Clique em qualquer objeto

Dentro do objeto, clique em: `Consulta com o S3 Select`

![Untitled](Imagens/Untitled%2081.png)

Em configura√ß√µes de entrada, selecione o formato: `Apache Parquet`

Em configura√ß√µes de sa√≠da, selecione o formato: `CSV`

![Captura da Web_14-3-2023_192012_us-east-1.console.aws.amazon.com.jpeg](Imagens/Captura_da_Web_14-3-2023_192012_us-east-1.console.aws.amazon.com.jpeg)

Em consulta SQL, selecione: `Executar consulta SQL`

Em resultados da consulta, verifique se o resultado da consulta, batem com os mesmos dados gerados pelo Producer.

![Untitled](Imagens/Untitled%2082.png)

<br>

## Criando e fazendo consultas com o Athena <a name = "Athena"></a>

Para que √© usado o servi√ßo Athena? Com o Athena √© poss√≠vel analisar dados, utilizando consultas em SQL, √© poss√≠vel utilizar como fonte de dados o S3 e outros servi√ßos

V√° no painel e na pesquisa procure e clique no servi√ßo Athena

![Untitled](Imagens/Untitled%2083.png)

Selecione: `Query your data`

![Untitled](Imagens/Untitled%2084.png)

Verifique se o database selecionado √© o que foi criado anteriormente

Verifique se a tabela selecionada √© a que foi criada anteriormente

Fa√ßa uma consulta simples, verificando se est√° tudo funcionando corretamente

```sql
SELECT * FROM wildrydes_table
```

![Untitled](Imagens/Untitled%2085.png)

Clique em: `Run`

![Untitled](Imagens/Untitled%2086.png)
